{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 1: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is election year for the United States of America, the Democratic nominee (Joe Biden) has open up to shortlisting companies capable of running analysis on social media platforms to inform his campaign of the public sentiments towards himself as well as his party [Source](https://www.nytimes.com/news-event/2020-election). His competition, President Trump has been utilising social media platforms; specifically twitter and has a higher presence in the digital space. On President Trump's own Facebook page, he has more than 29 million followers compared to Joe Biden's 2 million followers. Twitter tells a similar story; about 80 million for Trump, 5.4 million for Biden [Source](https://www.npr.org/2020/05/21/859932268/trump-and-biden-wage-an-uneven-virtual-campaign). \n",
    "\n",
    "As technology becomes intertwined with our daily living, it becomes inevitable that these platforms becomes more prominent in it influence for elections [Source](https://journalism.uoregon.edu/news/six-ways-media-influences-elections). Hence, it has become increasing important to understand the social media landscape to further improve one's chance when election day arrives. The campaign liaison has specified there are two categorise of competitions for companies to apply for tender. In the first category the model must be capable of classifying whether a block of text is generally democrat leaning or republican leaning. In the second category the model must be able to do sentiment analysis to understand if the block of text is positive or negative. Our team decided to focus on the first category.\n",
    "\n",
    "We choose the reddit as our platform of choice and from there we would like to uncover the answers to the following problems:  \n",
    "\n",
    "- **Between the two similar subreddits r/democrats and r/Republican are able to differentiate the post using Natural Language Processing models?**  \n",
    "- **Which models is then likely to work best?**\n",
    "\n",
    "Success is evaluated based on answering the problem statements and producing a model that has the highest classification score base upon metrics like accuracy, precision, recall and f1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project sets out to propose a classification modeling approach that would most accurately enable the us to predict which subreddit a post belongs to, using only the title and post data with subreddit names removed. This will assist in our application for the competition tender with the democratic nominee campaign. To illustrate this process and build a proposal using the selected platform Reddit, we have selected to build, evaluate, and compare classification models using Natural Language Processing (NLP) tools.  \n",
    "\n",
    "Our chosen subreddits to compare are:\n",
    "\n",
    " - **r/democrats**\n",
    " - **r/Republican**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "- Gather and prepare your data using the `requests` library.\n",
    "- **Create and compare two models**. One of these must be a Bayes classifier, however the other can be a classifier of your choosing: logistic regression, KNN, SVM, etc.\n",
    "- A Jupyter Notebook with your analysis for a peer audience of data scientists.\n",
    "- An executive summary of the results you found.\n",
    "- A short presentation outlining your process and findings for a semi-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Function to pull post using Reddit API](#Function-to-pull-post-using-Reddit-API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pull post using Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit is able to rank post via top and further filter to include all posts since the subreddit inception I decided to scrap the top 1000 post (limit of reddit architecture [sources](https://www.reddit.com/r/redditdev/comments/30a7ap/does_reddit_api_limit_total_listings_returned_to/) with the rationale that top posts are likely to have more content be it post length or comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create header parameter to prevent error 429\n",
    "headers = {'User-agent': 'dsi15-p3-mike'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap function for subreddit using the provided tutorial as reference\n",
    "def subreddit_scrap(subreddit, after, no_times):\n",
    "    default_url = 'https://old.reddit.com/'\n",
    "    subreddit = subreddit\n",
    "    ranking = '/top.json?t=all'\n",
    "\n",
    "    scrap_url = default_url + subreddit + ranking\n",
    "\n",
    "    after = after\n",
    "    posts = []\n",
    "\n",
    "    for i in range(no_times):\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else: \n",
    "            params = {'after': after}\n",
    "        \n",
    "        res = requests.get(scrap_url, params = params, headers = headers)\n",
    "    \n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            posts.extend(the_json['data']['children'])\n",
    "            after = the_json['data']['after']\n",
    "            unique_post = len(set([p['data']['name'] for p in posts]))\n",
    "            print('{} unique posts: '.format(subreddit), str(unique_post))\n",
    "            print('{} after: '.format(subreddit), after)\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "\n",
    "        time.sleep(randint(2, 6))\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a limit of 1000 posts due to the architeture of reddit\n",
    "dems_posts = subreddit_scrap('r/democrats', None, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems_posts_df = pd.DataFrame(dems_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems_posts_df.to_csv('../datasets/dems_top1000_posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_posts = subreddit_scrap('r/Republican', None, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_posts_df = pd.DataFrame(reps_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_posts_df.to_csv('../datasets/reps_top1000_posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue to Notebook 02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
